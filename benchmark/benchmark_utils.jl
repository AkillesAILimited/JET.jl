using Statistics
import Base.Meta: isexpr

"""
    ret = @freshexec [setup_ex = JET_SETUP_EX] ex

Runs `ex` in an external process and gets back the final result (, which is supposed to be
  such a simple Julia object that we can restore it from its string representation).
Running  in external process can be useful for testing JET analysis, because:
- the first time analysis is not affected by native code cache and JET's global report cache
- we can do something that might break a Julia process without wondering it breaks the
  original test process later on

`setup_ex` runs before each execution of `ex` and defaults to `JET_SETUP_EX`, which
  just loads JET into the process.
"""
macro freshexec(args...) freshexec(__module__, args...) end
freshexec(mod, ex) = freshexec(mod, JET_SETUP_EX, ex)
freshexec(args...) = _freshexec(args..., collect_last_result)

function collect_last_result(exs)
    lines = string.(exs)
    lines[end] = "ret = $(lines[end])"
    return join(lines, '\n')
end

"""
    stats = @benchmark_freshexec [setup_ex = JET_SETUP_EX] ex

Runs `ex` in an external process and collects execution statistics from [`@timed`](@ref).
This is particularly useful for testing the performance of first-time analysis, where
  the native code cache and JET's global report cache have no effect for its performance.
`setup_ex` runs before each execution of `ex` and it's not included in the benchmark result;
  `setup_ex` defaults to `JET_SETUP_EX`, which loads JET and runs warm up analysis
  `@profile identity(nothing)`.
"""
macro benchmark_freshexec(args...) benchmark_freshexec(__module__, args...) end
benchmark_freshexec(mod, ex) = benchmark_freshexec(mod, JET_SETUP_EX, ex)
benchmark_freshexec(args...) = _freshexec(args..., collect_statistics)

function collect_statistics(exs)
    return """
    ret = @timed begin
        $(join(exs, '\n'))
        nothing # ensure `stats` can be parsed
    end
    """
end

N_BENCHMARK_TIMES = 5

"""
    stats = @nbenchmark_freshexec [setup_ex = JET_SETUP_EX] ex

Runs `ex` in an external process multiple times (specified by `N_BENCHMARK_TIMES = $(N_BENCHMARK_TIMES)` global object),
  and collects execution statistics from [`@timed`](@ref). The statistics are generated by
  taking the mean of all the trials.
This is particularly useful for benchmarking the performance of first-time analysis, where
  the native code cache and JET's global report cache have no effect for its performance.
`setup_ex` runs before each execution of `ex` and it's not included in the benchmark result;
  `setup_ex` defaults to `JET_SETUP_EX`, which loads JET and runs warm up analysis
  `@profile identity(nothing)`.
"""
macro nbenchmark_freshexec(args...) nbenchmark_freshexec(__module__, args...) end
nbenchmark_freshexec(mod, ex) = nbenchmark_freshexec(mod, JET_SETUP_EX, ex)
function nbenchmark_freshexec(args...)
    stats = [_freshexec(args..., collect_statistics) for _ in 1:N_BENCHMARK_TIMES]
    return (; time   = mean(getproperty.(stats, :time)),
              bytes  = mean(getproperty.(stats, :bytes)),
              gctime = mean(getproperty.(stats, :gctime)),
              )
end

function _freshexec(mod, setup_ex, ex, exs2script)
    # we need to flatten block expression into a toplevel expression to correctly handle
    # e.g. macro expansions
    setup_exs = isexpr(setup_ex, :block) ? setup_ex.args : [setup_ex]
    setup_script = join(string.(setup_exs), '\n')
    exs = isexpr(ex, :block) ? ex.args : [ex]
    script = exs2script(exs)

    prog = """
    old = stdout
    rw, wr = redirect_stdout()
    $(setup_script)
    $(script)
    redirect_stdout(old)
    close(rw); close(wr)
    println(stdout, repr(ret))
    """

    cmd = Cmd([JULIA_BIN, "-e", prog])
    io = IOBuffer()
    run(pipeline(cmd; stdout = io))

    return Core.eval(mod, Meta.parse(String(take!(io))))
end

const JET_SETUP_EX = quote
    using JET
    @profile_call identity(nothing) # warm-up for JET analysis
end
const JULIA_BIN = normpath(Sys.BINDIR, "julia")
